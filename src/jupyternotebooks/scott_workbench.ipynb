{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Icevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting icevision\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/9e/fd2b5a829bd59f8165b837294f07337819d954a649ee4efcf9e39af9f3c9/icevision-0.8.1-py3-none-any.whl (224kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 2.2MB/s ta 0:00:01\r\n",
      "\u001b[?25hRequirement already satisfied: pillow>8.0.0 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from icevision) (8.2.0)\r\n",
      "Requirement already satisfied: matplotlib<4,>=3.2.2 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from icevision) (3.3.4)\r\n",
      "Requirement already satisfied: importlib-metadata>=1; python_version < \"3.8\" in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from icevision) (4.5.0)\r\n",
      "Collecting torch<1.9,>=1.7.0 (from icevision)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/07/46/82d6019ffd9be22ba6cb6ec4c271c86ed942a00a2052520657e8d32ac545/torch-1.8.1-cp36-none-macosx_10_9_x86_64.whl\r\n",
      "Collecting opencv-python<5,>=4.1.1 (from icevision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/e3/e5927583978ddfb5ff61f0d146bd1cac28a1f6b13c9880cc448b23e460d3/opencv_python-4.5.3.56-cp36-cp36m-macosx_10_15_x86_64.whl (42.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 42.6MB 738kB/s ta 0:00:011\r\n",
      "\u001b[?25hCollecting torchvision<0.10,>=0.8.0 (from icevision)\r\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/19/19ea4cec5a7048eb2596f1574bed5bfec703c0fe9155b1bad0d9b8d198e4/torchvision-0.9.1-cp36-cp36m-macosx_10_9_x86_64.whl\r\n",
      "Collecting pycocotools>=2.0.2<3 (from icevision)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/de/df/056875d697c45182ed6d2ae21f62015896fdb841906fe48e7268e791c467/pycocotools-2.0.2.tar.gz\r\n",
      "Collecting albumentations<0.6,>=0.4.6 (from icevision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 28.8MB/s ta 0:00:01\r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.24.0<3 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from icevision) (2.25.1)\r\n",
      "Collecting loguru>=0.5.3 (from icevision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 3.2MB/s ta 0:00:01\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.49.0 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from icevision) (4.61.1)\r\n",
      "Requirement already satisfied: fastcore<1.4,>=1.3.0 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from icevision) (1.3.20)\r\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from matplotlib<4,>=3.2.2->icevision) (1.19.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from matplotlib<4,>=3.2.2->icevision) (2.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from matplotlib<4,>=3.2.2->icevision) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from matplotlib<4,>=3.2.2->icevision) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from matplotlib<4,>=3.2.2->icevision) (1.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from importlib-metadata>=1; python_version < \"3.8\"->icevision) (3.10.0.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from importlib-metadata>=1; python_version < \"3.8\"->icevision) (3.4.1)\r\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from torch<1.9,>=1.7.0->icevision) (0.8)\r\n",
      "Requirement already satisfied: setuptools>=18.0 in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages/setuptools-40.8.0-py3.6.egg (from pycocotools>=2.0.2<3->icevision) (40.8.0)\r\n",
      "Collecting cython>=0.27.3 (from pycocotools>=2.0.2<3->icevision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/b1/e1d71346f8b04f85209e339faefdd07328ca6cf869ce276e47b23afdc72c/Cython-0.29.23-cp36-cp36m-macosx_10_9_x86_64.whl (1.9MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 1.9MB 3.2MB/s ta 0:00:01\r\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /Users/scott.zanevra/PycharmProjects/fastai-live-video-logo-obfuscation/venv/lib/python3.6/site-packages (from albumentations<0.6,>=0.4.6->icevision) (5.4.1)\r\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations<0.6,>=0.4.6->icevision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/bfc6d5787d53b56e0b40d9a54099e7dbdb04a890b45d8facf3f41fc5511a/opencv_python_headless-4.5.3.56-cp36-cp36m-macosx_10_15_x86_64.whl (42.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 42.6MB 984kB/s ta 0:00:011\r\n",
      "\u001b[?25hCollecting scikit-image>=0.16.1 (from albumentations<0.6,>=0.4.6->icevision)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/1f/bea69a3a5d7efb0e22993d08c4328678e5f6a513cad55247142be8473142/scikit_image-0.17.2-cp36-cp36m-macosx_10_13_x86_64.whl (12.1MB)\r\n",
      "\u001b[K    37% |████████████                    | 4.5MB 3.9MB/s eta 0:00:021  Found existing installation: torchvision 0.10.0\r\n",
      "    Uninstalling torchvision-0.10.0:\r\n",
      "      Successfully uninstalled torchvision-0.10.0\r\n",
      "  Running setup.py install for pycocotools ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Found existing installation: decorator 5.0.9\r\n",
      "    Uninstalling decorator-5.0.9:\r\n",
      "      Successfully uninstalled decorator-5.0.9\r\n",
      "Successfully installed PyWavelets-1.1.1 Shapely-1.7.1 aiocontextvars-0.2.2 albumentations-0.5.2 cython-0.29.23 decorator-4.4.2 icevision-0.8.1 imageio-2.9.0 imgaug-0.4.0 loguru-0.5.3 networkx-2.5.1 opencv-python-4.5.3.56 opencv-python-headless-4.5.3.56 pycocotools-2.0.2 scikit-image-0.17.2 tifffile-2020.9.3 torch-1.8.1 torchvision-0.9.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip install icevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "\u001b[1m\u001b[1mINFO    \u001b[0m\u001b[1m\u001b[0m - \u001b[1mDownloading default `.ttf` font file - SpaceGrotesk-Medium.ttf from https://raw.githubusercontent.com/airctic/storage/master/SpaceGrotesk-Medium.ttf to /Users/scott.zanevra/.icevision/fonts/SpaceGrotesk-Medium.ttf\u001b[0m | \u001b[36micevision.visualize.utils\u001b[0m:\u001b[36mget_default_font\u001b[0m:\u001b[36m69\u001b[0m\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from icevision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Mike Logo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "/home/ubuntu/nikelogos\n",
      "<class 'pathlib.PosixPath'>\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05128d48d714498b81ddfce2d11171ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": [
      "\u001b[1m\u001b[1mINFO    \u001b[0m\u001b[1m\u001b[0m - \u001b[1m\u001b[34m\u001b[1mAutofixing records\u001b[0m\u001b[1m\u001b[34m\u001b[0m\u001b[1m\u001b[0m | \u001b[36micevision.parsers.parser\u001b[0m:\u001b[36mparse\u001b[0m:\u001b[36m136\u001b[0m\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "807f474f2494452e845ef2e2139b5574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bc90f037b5d44068cc29b6a0cadee19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(0, 0)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "# Location of the image root\n",
    "# Update the path of the nikelogs\n",
    "data_dir = Path('/home/ubuntu/nikelogos')\n",
    "print(data_dir)\n",
    "print(type(data_dir))\n",
    "\n",
    "# Create the parser\n",
    "parser = parsers.VOCBBoxParser(annotations_dir=data_dir / \"annotations\", images_dir=data_dir / \"images\")\n",
    "parser.class_map\n",
    "\n",
    "\n",
    "# Parse annotations to create records\n",
    "train_records, valid_records = parser.parse()\n",
    "\n",
    "# Transforms\n",
    "# size is set to 384 because EfficientDet requires its inputs to be divisible by 128\n",
    "image_size = 384\n",
    "train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=512), tfms.A.Normalize()])\n",
    "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])\n",
    "\n",
    "# Datasets\n",
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)\n",
    "\n",
    "len(train_ds), len(valid_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the transforms\n",
    "\n",
    "The Dataset transforms are only applied when we grab (get) an item. Several of the default `aug_tfms` have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation  is randomly selected between +45 and -45 degrees.\n",
    "\n",
    "This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning.\n",
    "\n",
    "We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show an element of the train_ds with augmentation transformations applied\n",
    "samples = [train_ds[0] for _ in range(3)]\n",
    "show_samples(samples, ncols=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select a library, model, and backbone\n",
    "\n",
    "In order to create a model, we need to:\n",
    "* Choose one of the **libraries** supported by IceVision\n",
    "* Choose one of the **models** supported by the library\n",
    "* Choose one of the **backbones** corresponding to a chosen model\n",
    "\n",
    "You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a model\n",
    "Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone  could be specified by:\n",
    "```\n",
    "model_type = models.mmdet.retinanet\n",
    "backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True)\n",
    "```\n",
    "As pretrained models are used by default, we typically leave this out of the backbone creation step.\n",
    "\n",
    "We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of `selection`. This shows you how easy it is to try new libraries, models, and backbones.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just change the value of selection to try another model\n",
    "\n",
    "selection = 0\n",
    "\n",
    "\n",
    "extra_args = {}\n",
    "\n",
    "if selection == 0:\n",
    "  model_type = models.mmdet.retinanet\n",
    "  backbone = model_type.backbones.resnet50_fpn_1x\n",
    "\n",
    "elif selection == 1:\n",
    "  # The Retinanet model is also implemented in the torchvision library\n",
    "  model_type = models.torchvision.retinanet\n",
    "  backbone = model_type.backbones.resnet50_fpn\n",
    "\n",
    "elif selection == 2:\n",
    "  model_type = models.ross.efficientdet\n",
    "  backbone = model_type.backbones.tf_lite0\n",
    "  # The efficientdet model requires an img_size parameter\n",
    "  extra_args['img_size'] = image_size\n",
    "\n",
    "elif selection == 3:\n",
    "  model_type = models.ultralytics.yolov5\n",
    "  backbone = model_type.backbones.small\n",
    "  # The yolov5 model requires an img_size parameter\n",
    "  extra_args['img_size'] = image_size\n",
    "\n",
    "model_type, backbone, extra_args"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now it is just a one-liner to instantiate the model. If you want to try another *option*, just edit the line at the top of the previous cell.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the mdoel\n",
    "model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loader\n",
    "\n",
    "The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets.\n",
    "\n",
    "We can take a look at the first batch of items from the `valid_dl`. Remember that the `valid_tfms` only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_dl = model_type.train_dl(train_ds, batch_size=8, num_workers=4, shuffle=True)\n",
    "valid_dl = model_type.valid_dl(valid_ds, batch_size=8, num_workers=4, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show batch\n",
    "model_type.show_batch(first(valid_dl), ncols=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics\n",
    "\n",
    "The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries.\n",
    "\n",
    "The same metrics can be used for both fastai and pytorch lightning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as [fastai2](https://github.com/fastai/fastai2), and [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning).  \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training using fastai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "\n",
    "# For Sparse-RCNN, use lower `end_lr`\n",
    "# learn.lr_find(end_lr=0.005)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn.fine_tune(20, 1e-4, freeze_epochs=1)\n",
    "\n",
    "# 40, 1e-4\n",
    "# 39\t0.293704\t0.531666\t0.355570\t00:15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training using Pytorch Lightning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LightModel(model_type.lightning.ModelAdapter):\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.parameters(), lr=1e-4)\n",
    "    \n",
    "light_model = LightModel(model, metrics=metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=20, gpus=1)\n",
    "trainer.fit(light_model, train_dl, valid_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the model - inference and showing results\n",
    "\n",
    "The first step in reviewing the model is to show results from the validation dataset. This is easy to do with the `show_results` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_type.show_results(model, valid_ds, detection_threshold=.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction\n",
    "\n",
    "Sometimes you want to have more control than `show_results` provides. You can construct an inference dataloader using `infer_dl` from any IceVision dataset and pass this to `predict_dl` and use `show_preds` to look at the predictions.\n",
    "\n",
    "A prediction is returned as a dict with keys: `scores`, `labels`, `bboxes`, and possibly `masks`. \n",
    "\n",
    "Prediction functions that take a `detection_threshold` argument will only return the predictions whose score is above the threshold.\n",
    "\n",
    "Prediction functions that take a `keep_images` argument will only return the (tensor representation of the) image when it is `True`. In interactive environments, such as a notebook, it is helpful to see the image with bounding boxes and labels applied. In a deployment context, however, it is typically more useful (and efficient) to return the bounding boxes by themselves.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> NOTE: For a more detailed look at inference check out the [inference tutorial](https://airctic.com/dev/inference/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "infer_dl = model_type.infer_dl(valid_ds, batch_size=4, shuffle=False)\n",
    "preds = model_type.predict_from_dl(model, infer_dl, keep_images=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_preds(preds=preds[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}