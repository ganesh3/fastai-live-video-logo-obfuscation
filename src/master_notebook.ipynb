{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install icevision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from icevision.all import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Location of the image root\n",
    "data_dir = Path('/home/ubuntu/nikelogos')\n",
    "print(data_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parse the dataset\n",
    "\n",
    "The parser loads the annotation file and parses them returning a list of training and validation records. The parser has an extensible `autofix` capability that identifies common errors in annotation files, reports, and often corrects them automatically.\n",
    "\n",
    "The parsers support multiple formats (including VOC and COCO). You can also extend the parser for additional formats if needed.\n",
    "\n",
    "The record is a key concept in IceVision, it holds the information about an image and its annotations. It is extensible and can support other object formats and types of annotations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the parser\n",
    "parser = parsers.VOCBBoxParser(annotations_dir=data_dir / \"annotations\", images_dir=data_dir / \"images\")\n",
    "parser.class_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parse annotations to create records\n",
    "train_records, valid_records = parser.parse()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating datasets with agumentations and transforms\n",
    "\n",
    "Data augmentations are essential for robust training and results on many datasets and deep learning tasks. IceVision ships with the [Albumentations](https://albumentations.ai/docs/) library for defining and executing transformations, but can be extended to use others.\n",
    "\n",
    "For this tutorial, we apply the Albumentation's default `aug_tfms` to the training set. `aug_tfms` randomly applies broadly useful transformations including rotation, cropping, horizintal flips, and more. See the Albumentations documentation to learn how to customize each transformation more fully.\n",
    "\n",
    "The validation set is only resized (with padding).\n",
    "\n",
    "We then create `Datasets` for both. The dataset applies the transforms to the annotations (such as bounding boxes) and images in the data records."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transforms\n",
    "# size is set to 384 because EfficientDet requires its inputs to be divisible by 128\n",
    "image_size = 384\n",
    "train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=512), tfms.A.Normalize(), tfms.A.Rotate()])\n",
    "valid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_ds = Dataset(train_records, train_tfms)\n",
    "valid_ds = Dataset(valid_records, valid_tfms)\n",
    "print(valid_ds)\n",
    "print(train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Understanding the transforms\n",
    "\n",
    "The Dataset transforms are only applied when we grab (get) an item. Several of the default `aug_tfms` have a random element to them. For example, one might perform a rotation with probability 0.5 where the angle of rotation  is randomly selected between +45 and -45 degrees.\n",
    "\n",
    "This means that the learner sees a slightly different version of an image each time it is accessed. This effectively increases the size of the dataset and improves learning.\n",
    "\n",
    "We can look at result of getting the 0th image from the dataset a few times and see the differences. Each time you run the next cell, you will see different results due to the random element in applying transformations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show an element of the train_ds with augmentation transformations applied\n",
    "samples = [train_ds[0] for _ in range(6)]\n",
    "show_samples(samples, ncols=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select a library, model, and backbone\n",
    "\n",
    "In order to create a model, we need to:\n",
    "* Choose one of the **libraries** supported by IceVision\n",
    "* Choose one of the **models** supported by the library\n",
    "* Choose one of the **backbones** corresponding to a chosen model\n",
    "\n",
    "You can access any supported models by following the IceVision unified API, use code completion to explore the available models for each library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a model\n",
    "Selections only take two simple lines of code. For example, to try the mmdet library using the retinanet model and the resnet50_fpn_1x backbone  could be specified by:\n",
    "```\n",
    "model_type = models.mmdet.retinanet\n",
    "backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True)\n",
    "```\n",
    "As pretrained models are used by default, we typically leave this out of the backbone creation step.\n",
    "\n",
    "We've selected a few of the many options below. You can easily pick which option you want to try by setting the value of `selection`. This shows you how easy it is to try new libraries, models, and backbones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Just change the value of selection to try another model\n",
    "\n",
    "selection = 1\n",
    "\n",
    "\n",
    "extra_args = {}\n",
    "\n",
    "if selection == 0:\n",
    "  model_type = models.mmdet.retinanet\n",
    "  backbone = model_type.backbones.resnet50_fpn_1x\n",
    "\n",
    "elif selection == 1:\n",
    "  # The Retinanet model is also implemented in the torchvision library\n",
    "  model_type = models.torchvision.retinanet\n",
    "  backbone = model_type.backbones.resnet50_fpn\n",
    "\n",
    "elif selection == 2:\n",
    "  model_type = models.ross.efficientdet\n",
    "  backbone = model_type.backbones.tf_lite0\n",
    "  # The efficientdet model requires an img_size parameter\n",
    "  extra_args['img_size'] = image_size\n",
    "\n",
    "elif selection == 3:\n",
    "  model_type = models.ultralytics.yolov5\n",
    "  backbone = model_type.backbones.small\n",
    "  # The yolov5 model requires an img_size parameter\n",
    "  extra_args['img_size'] = image_size\n",
    "\n",
    "model_type, backbone, extra_args"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it is just a one-liner to instantiate the model. If you want to try another *option*, just edit the line at the top of the previous cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the mdoel\n",
    "model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loader\n",
    "\n",
    "The Data Loader is specific to a model_type. The job of the data loader is to get items from a dataset and batch them up in the specific format required by each model. This is why creating the data loaders is separated from creating the datasets.\n",
    "\n",
    "We can take a look at the first batch of items from the `valid_dl`. Remember that the `valid_tfms` only resized (with padding) and normalized records, so different images, for example, are not returned each time. This is important to provide consistent validation during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "train_dl = model_type.train_dl(train_ds, batch_size=8, num_workers=4, shuffle=True)\n",
    "valid_dl = model_type.valid_dl(valid_ds, batch_size=8, num_workers=4, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show batch\n",
    "model_type.show_batch(first(valid_dl), ncols=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics\n",
    "\n",
    "The fastai and pytorch lightning engines collect metrics to track progress during training. IceVision provides metric classes that work across the engines and libraries.\n",
    "\n",
    "The same metrics can be used for both fastai and pytorch lightning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "IceVision is an agnostic framework meaning it can be plugged into other DL learning engines such as [fastai2](https://github.com/fastai/fastai2), and [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Find Learning Rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# learn.fine_tune(40, 1e-4, freeze_epochs=1)\n",
    "num_epochs = 100\n",
    "learn.fine_tune(num_epochs, 1e-3, freeze_epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Model Outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PATH = f\"nikemodel_model_{selection}_new_{num_epochs}.mm\"\n",
    "torch.save(model.state_dict(), PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Model Outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import PIL\n",
    "img_path = \"image1.jpeg\"\n",
    "# img_path = \"nike_female_jumper.png\"\n",
    "# img_path = \"image_2.jpg\"\n",
    "img = PIL.Image.open(img_path)\n",
    "infer_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size=384), tfms.A.Normalize()])\n",
    "infer_ds = Dataset.from_images([img], infer_tfms)\n",
    "\n",
    "preds = model_type.predict(model, infer_ds, keep_images=True)\n",
    "for x in preds[0].pred.detection.components:\n",
    "    print(x)\n",
    "    if 'ScoresRecordComponent' in str(x):\n",
    "        scores = x.scores\n",
    "        print(scores)\n",
    "    if 'InstancesLabelsRecordComponent' in str(x):\n",
    "        labels = x.label_ids\n",
    "        print(labels)\n",
    "    if 'BBoxesRecordComponen' in str(x):\n",
    "        bboxes = x.bboxes\n",
    "        print(bboxes)\n",
    "        \n",
    "show_preds(preds=preds[0:1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}